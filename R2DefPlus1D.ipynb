{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Conv2d\n",
    "from torch import Tensor\n",
    "from torchvision.ops import DeformConv2d\n",
    "from typing import Any, Callable, List, Optional, Sequence, Tuple, Type, Union\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"Trains//R2+1D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory does not exist.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"runs/R2+1D\"):\n",
    "    print(\"Log directory does not exist.\")\n",
    "else:\n",
    "    print(\"Log directory exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/222 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [00:00<00:00, 6302.57it/s]\n"
     ]
    }
   ],
   "source": [
    "def input_cleaner(path=r'C:\\Users\\User\\Desktop\\Dataset2_cleaned'):\n",
    "    del_list = []\n",
    "    for folder in tqdm.tqdm(os.listdir(path)):\n",
    "        if len(os.listdir(path + '\\\\' + folder)) != 8:\n",
    "            del_list.append(path + '\\\\' + folder)\n",
    "    return del_list\n",
    "del_list = input_cleaner()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(del_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in del_list:\n",
    "    for file in os.listdir(folder):\n",
    "        os.remove(folder + '\\\\' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir(r'C:\\Users\\User\\Desktop\\Dataset2_cleaned'):\n",
    "    if not os.listdir(r'C:\\Users\\User\\Desktop\\Dataset2_cleaned' + '\\\\' + folder):\n",
    "        os.rmdir(r'C:\\Users\\User\\Desktop\\Dataset2_cleaned' + '\\\\' + folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {\n",
    "    'Car': torch.tensor(0),\n",
    "    'Clear': torch.tensor(1),\n",
    "    'Human': torch.tensor(2),\n",
    "    'LineNoise': torch.tensor(1),\n",
    "    'Noise': torch.tensor(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_label_dict = {\n",
    "    0: 'Car',\n",
    "    1: 'Noise',\n",
    "    2: 'Human'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_maker(path=r'C:\\Users\\User\\Desktop\\Dataset2_cleaned'):\n",
    "    video_lib = []\n",
    "    labels = []\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989])\n",
    "        ])\n",
    "    \n",
    "    for folder in tqdm.tqdm(os.listdir(path)):\n",
    "        frames = []\n",
    "        labels.append(labels_dict[folder.split(\",\")[0][2:-1]])\n",
    "        for img_path in os.listdir(path + '\\\\' + folder):\n",
    "            img = Image.open(path + '\\\\' + folder + '\\\\' + img_path)\n",
    "            frames.append(transform(img))\n",
    "        \n",
    "        # video_tensor = torch.stack(frames, dim=1)\n",
    "        # video_tensor.unsqueeze(0)\n",
    "\n",
    "        video_lib.append(frames)\n",
    "    \n",
    "    return video_lib, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [00:01<00:00, 138.75it/s]\n"
     ]
    }
   ],
   "source": [
    "video_data, labels = data_maker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222, 222)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(video_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_tensors, labels):\n",
    "        self.video_tensors = video_tensors\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        video = torch.tensor(np.array(self.video_tensors[index]))\n",
    "        label = torch.tensor(self.labels[index])\n",
    "        return video, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VideoDataset(video_data, labels)\n",
    "    \n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "val_size = int(0.2 * train_size)\n",
    "train_size = train_size - val_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = 8,\n",
    "    shuffle = True,    \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset = val_dataset,\n",
    "    batch_size = 8,\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = 8,\n",
    "    shuffle = False,\n",
    "    num_workers = 0\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e0ffa31dd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R(2def+1)D Model Defineing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedDeformConv2d(DeformConv2d):\n",
    "    @staticmethod\n",
    "    def get_downsample_stride(stride: int) -> Tuple[int, int, int]:\n",
    "        return stride, stride, stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeformableFrameConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        # Offset convolution to learn deformable convolution parameters\n",
    "        self.offset_conv = nn.Conv2d(\n",
    "            in_channels, \n",
    "            2 * kernel_size * kernel_size,  # 2 coordinates for x and y\n",
    "            kernel_size=kernel_size, \n",
    "            stride=stride, \n",
    "            padding=padding\n",
    "        )\n",
    "        \n",
    "        self.deform_conv = DeformConv2d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            stride=stride, \n",
    "            padding=padding \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, channels, frames, height, width]\n",
    "        batch_size, channels, frames, height, width = x.shape\n",
    "        \n",
    "        # Will store processed frames\n",
    "        output = []\n",
    "        \n",
    "        for frame in range(frames):\n",
    "            current_frame = x[:, :, frame, :, :]\n",
    "            \n",
    "            offset = self.offset_conv(current_frame)\n",
    "            \n",
    "            deformed_frame = self.deform_conv(current_frame, offset)\n",
    "            \n",
    "            output.append(deformed_frame)\n",
    "        \n",
    "        return torch.stack(output, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDeformableBlock(nn.Module):\n",
    "\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        conv_builder: Callable[..., nn.Module],\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "    ) -> None:\n",
    "        midplanes = int((planes + inplanes) / 2)\n",
    "        print(f\"planes {planes}, inplanes {inplanes}\")\n",
    "        super().__init__()\n",
    "        self.offset_conv1 = nn.Conv2d(\n",
    "            inplanes, \n",
    "            2 * 3 * 3,  # 2 coordinates for x and y\n",
    "            kernel_size=3, \n",
    "            stride=stride, \n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        self.deform_conv1 = conv_builder(\n",
    "            inplanes,\n",
    "            midplanes,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        self.temp_conv0 = nn.Sequential(\n",
    "            nn.Conv3d(inplanes, planes, kernel_size=(3, 1, 1), stride=(stride, 1, 1), padding=(1, 0, 0)), \n",
    "            nn.BatchNorm3d(planes), \n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.temp_conv1 = nn.Sequential(\n",
    "            nn.Conv3d(midplanes, midplanes, kernel_size=(3, 1, 1), stride=(stride, 1, 1), padding=(1, 0, 0)), \n",
    "            nn.BatchNorm3d(planes), \n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.offset_conv2 = nn.Conv2d(\n",
    "            midplanes,\n",
    "            2 * 3 * 3,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        self.deform_conv2 = conv_builder(\n",
    "            midplanes,\n",
    "            planes,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        self.temp_conv2 = nn.Sequential(\n",
    "            nn.Conv3d(planes, planes, kernel_size=(3, 1, 1), stride=(stride, 1, 1), padding=(1, 0, 0)), \n",
    "            nn.BatchNorm3d(planes), \n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        print(x.shape)\n",
    "        residual = self.temp_conv0(x)\n",
    "        print(\"residual shape\", residual.shape)\n",
    "\n",
    "        # x shape: [batch_size, channels, frames, height, width]\n",
    "        batch_size, channels, frames, height, width = x.shape\n",
    "        \n",
    "        # Will store processed frames\n",
    "        first_output = torch.zeros(\n",
    "            batch_size, self.deform_conv1.out_channels, frames, height, width\n",
    "        )\n",
    "        print(\"going to first loop\")\n",
    "        for frame in range(frames):\n",
    "            current_frame = x[:, :, frame, :, :]\n",
    "            \n",
    "            offset1 = self.offset_conv1(current_frame)\n",
    "            \n",
    "            deformed_frame = self.deform_conv1(current_frame, offset1)\n",
    "            \n",
    "            first_output[:, :, frame, :, :] = deformed_frame\n",
    "        print(\"first output shape is\", first_output.shape)\n",
    "        out = first_output\n",
    "        # out = torch.stack(output, dim=2)\n",
    "        # out = self.temp_conv1(x)\n",
    "\n",
    "        batch_size, channels, frames, height, width = out.shape\n",
    "        second_output = torch.zeros(\n",
    "            batch_size, self.deform_conv2.out_channels, frames, height, width\n",
    "        )\n",
    "        for frame in range(frames):\n",
    "            current_frame = out[:, :, frame, :, :]\n",
    "            \n",
    "            offset2 = self.offset_conv2(current_frame)\n",
    "            \n",
    "            deformed_frame = self.deform_conv2(current_frame, offset2)\n",
    "            \n",
    "            second_output[:, :, frame, :, :] = deformed_frame\n",
    "        print(\"second output shape is\", second_output.shape)\n",
    "        out = second_output\n",
    "        # out = torch.stack(output, dim=2)\n",
    "        # out = self.temp_conv2(out)\n",
    "        # out = self.conv2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2Plus1dStem(nn.Sequential):\n",
    "    \"\"\"R(2+1)D stem is different than the default one as it uses separated 3D convolution\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\n",
    "            nn.Conv3d(3, 45, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False),\n",
    "            nn.BatchNorm3d(45),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(45, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoResNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            block: Type[Union[BasicDeformableBlock]],\n",
    "            conv_makers: Sequence[Type[Union[ExtendedDeformConv2d]]],\n",
    "            nums_of_layers: List[int], \n",
    "            stem: Callable[..., nn.Module],\n",
    "            num_classes: int = 2,\n",
    "            zero_init_residual: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.inplanes = 64\n",
    "\n",
    "        self.stem = stem()\n",
    "\n",
    "        self.layer1 = self._make_layer(64, block, conv_makers[0], 128, nums_of_layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(128, block, conv_makers[1], 256, nums_of_layers[1], stride=1)\n",
    "        self.layer3 = self._make_layer(256, block, conv_makers[2], 512, nums_of_layers[2], stride=1)\n",
    "        self.layer4 = self._make_layer(512, block, conv_makers[3], 1024, nums_of_layers[3], stride=1)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(1024 * block.expansion, num_classes)\n",
    "\n",
    "        #init weights \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        # if zero_init_residual:\n",
    "        #     for m in self.modules():\n",
    "        #         if isinstance(m, Bottleneck):\n",
    "        #             nn.init.constant_(m.bn3.weight, 0)\n",
    "        \n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        #flatten layer to fc\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _make_layer(\n",
    "            self,\n",
    "            inplanes,\n",
    "            block: Type[Union[BasicDeformableBlock]],\n",
    "            conv_builder: Type[Union[ExtendedDeformConv2d]],\n",
    "            out_planes: int,\n",
    "            num_of_blocks: int,\n",
    "            stride: int = 1,\n",
    "            ) -> nn.Sequential:\n",
    "        downsample = None\n",
    "\n",
    "        if stride != 1 or self.inplanes != out_planes *  block.expansion:\n",
    "            ds_stride = conv_builder.get_downsample_stride(stride)\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv3d(self.inplanes, out_planes*block.expansion, kernel_size=1, stride=ds_stride, bias=False),\n",
    "                nn.BatchNorm3d(out_planes * block.expansion)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(inplanes, out_planes, conv_builder, stride, downsample))\n",
    "\n",
    "        self.inplanes = out_planes * block.expansion\n",
    "        for i in range(1, num_of_blocks):\n",
    "            layers.append(block(self.inplanes, out_planes, conv_builder))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video_resnet(\n",
    "    block: Type[Union[BasicDeformableBlock]],\n",
    "    conv_makers: Sequence[Type[Union[ExtendedDeformConv2d]]],\n",
    "    layers: List[int],\n",
    "    stem: Callable[..., nn.Module],\n",
    ") -> VideoResNet:\n",
    "\n",
    "    model = VideoResNet(block, conv_makers, layers, stem)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planes 128, inplanes 64\n",
      "planes 128, inplanes 128\n",
      "planes 256, inplanes 128\n",
      "planes 256, inplanes 256\n",
      "planes 512, inplanes 256\n",
      "planes 512, inplanes 512\n",
      "planes 1024, inplanes 512\n",
      "planes 1024, inplanes 1024\n"
     ]
    }
   ],
   "source": [
    "model = make_video_resnet(\n",
    "        BasicDeformableBlock,\n",
    "        [ExtendedDeformConv2d] * 4,\n",
    "        [2, 2, 2, 2],\n",
    "        R2Plus1dStem,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18648\\725134716.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(self.labels[index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 64, 8, 56, 56])\n",
      "residual shape torch.Size([8, 128, 8, 56, 56])\n",
      "going to first loop\n",
      "first output shape is torch.Size([8, 96, 8, 56, 56])\n",
      "second output shape is torch.Size([8, 128, 8, 56, 56])\n",
      "torch.Size([8, 128, 8, 56, 56])\n",
      "residual shape torch.Size([8, 128, 8, 56, 56])\n",
      "going to first loop\n",
      "first output shape is torch.Size([8, 128, 8, 56, 56])\n",
      "second output shape is torch.Size([8, 128, 8, 56, 56])\n",
      "torch.Size([8, 128, 8, 56, 56])\n",
      "residual shape torch.Size([8, 256, 8, 56, 56])\n",
      "going to first loop\n",
      "first output shape is torch.Size([8, 192, 8, 56, 56])\n",
      "second output shape is torch.Size([8, 256, 8, 56, 56])\n",
      "torch.Size([8, 256, 8, 56, 56])\n",
      "residual shape torch.Size([8, 256, 8, 56, 56])\n",
      "going to first loop\n",
      "first output shape is torch.Size([8, 256, 8, 56, 56])\n",
      "second output shape is torch.Size([8, 256, 8, 56, 56])\n",
      "torch.Size([8, 256, 8, 56, 56])\n",
      "residual shape torch.Size([8, 512, 8, 56, 56])\n",
      "going to first loop\n",
      "first output shape is torch.Size([8, 384, 8, 56, 56])\n",
      "second output shape is torch.Size([8, 512, 8, 56, 56])\n",
      "torch.Size([8, 512, 8, 56, 56])\n",
      "residual shape torch.Size([8, 512, 8, 56, 56])\n",
      "going to first loop\n",
      "first output shape is torch.Size([8, 512, 8, 56, 56])\n",
      "second output shape is torch.Size([8, 512, 8, 56, 56])\n",
      "torch.Size([8, 512, 8, 56, 56])\n",
      "residual shape torch.Size([8, 1024, 8, 56, 56])\n",
      "going to first loop\n",
      "first output shape is torch.Size([8, 768, 8, 56, 56])\n",
      "second output shape is torch.Size([8, 1024, 8, 56, 56])\n",
      "torch.Size([8, 1024, 8, 56, 56])\n",
      "residual shape torch.Size([8, 1024, 8, 56, 56])\n",
      "going to first loop\n",
      "first output shape is torch.Size([8, 1024, 8, 56, 56])\n",
      "second output shape is torch.Size([8, 1024, 8, 56, 56])\n",
      "tensor([[-0.3900,  0.0081],\n",
      "        [-0.3894,  0.0021],\n",
      "        [-0.4147,  0.0046],\n",
      "        [-0.3899,  0.0103],\n",
      "        [-0.3852,  0.0126],\n",
      "        [-0.3794,  0.0137],\n",
      "        [-0.3829,  0.0013],\n",
      "        [-0.3759,  0.0134]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for vid, _ in train_loader:\n",
    "    vid1 = vid.permute(0, 2, 1, 3, 4)\n",
    "    print(model(vid1))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"conv2_plus1d\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, None, None, None,  36864     \n",
      "                              64)                                \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, None, None, None,  256      \n",
      " ormalization)                64)                                \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, None, None, None,  0         \n",
      "                              64)                                \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, None, None, None,  24576     \n",
      "                              128)                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61,696\n",
      "Trainable params: 61,568\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from typing import Tuple\n",
    "\n",
    "class Conv2Plus1D(tf.keras.Sequential):\n",
    "    def __init__(\n",
    "        self, in_planes: int, out_planes: int, midplanes: int, stride: int = 1, padding: int = 1\n",
    "    ) -> None:\n",
    "        layers_list = [\n",
    "            layers.Conv3D(\n",
    "                filters=midplanes,\n",
    "                kernel_size=(1, 3, 3),\n",
    "                strides=(1, stride, stride),\n",
    "                padding='same' if padding > 0 else 'valid',\n",
    "                use_bias=False,\n",
    "                input_shape=(None, None, None, in_planes),\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv3D(\n",
    "                filters=out_planes,\n",
    "                kernel_size=(3, 1, 1),\n",
    "                strides=(stride, 1, 1),\n",
    "                padding='same' if padding > 0 else 'valid',\n",
    "                use_bias=False,\n",
    "            ),\n",
    "        ]\n",
    "        super().__init__(layers_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_downsample_stride(stride: int) -> Tuple[int, int, int]:\n",
    "        return stride, stride, stride\n",
    "\n",
    "conv_layer = Conv2Plus1D(in_planes=64, out_planes=128, midplanes=64, stride=2, padding=1)\n",
    "print(conv_layer.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"r2_plus1d_stem\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_2 (Conv3D)           (None, None, None, None,  6615      \n",
      "                              45)                                \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, None, None, None,  180      \n",
      " hNormalization)              45)                                \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, None, None, None,  0         \n",
      "                              45)                                \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, None, None, None,  8640      \n",
      "                              64)                                \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, None, None, None,  256      \n",
      " hNormalization)              64)                                \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, None, None, None,  0         \n",
      "                              64)                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,691\n",
      "Trainable params: 15,473\n",
      "Non-trainable params: 218\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class R2Plus1dStem(tf.keras.Sequential):\n",
    "    \"\"\"R(2+1)D stem is different than the default one as it uses separated 3D convolution\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        layers_list = [\n",
    "            layers.Conv3D(\n",
    "                filters=45,\n",
    "                kernel_size=(1, 7, 7),\n",
    "                strides=(1, 2, 2),\n",
    "                padding='same',  # Corresponds to PyTorch padding=(0, 3, 3)\n",
    "                use_bias=False,\n",
    "                input_shape=(None, None, None, 3),  # Input shape for the first layer\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv3D(\n",
    "                filters=64,\n",
    "                kernel_size=(3, 1, 1),\n",
    "                strides=(1, 1, 1),\n",
    "                padding='same',  # Corresponds to PyTorch padding=(1, 0, 0)\n",
    "                use_bias=False,\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ]\n",
    "        super().__init__(layers_list)\n",
    "\n",
    "# Example usage\n",
    "stem_layer = R2Plus1dStem()\n",
    "print(stem_layer.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 56, 56, 512)\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "class Bottleneck(tf.keras.layers.Layer):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        out_planes: int,\n",
    "        conv_builder: Callable[..., tf.keras.layers.Layer],\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[tf.keras.layers.Layer] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        midplanes = (inplanes * out_planes * 3 * 3 * 3) // (inplanes * 3 * 3 + 3 * out_planes)\n",
    "\n",
    "        self.conv1 = tf.keras.Sequential([\n",
    "            layers.Conv3D(\n",
    "                filters=out_planes,\n",
    "                kernel_size=1,\n",
    "                strides=1,\n",
    "                padding='valid',\n",
    "                use_bias=False\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "\n",
    "        self.conv2 = tf.keras.Sequential([\n",
    "            conv_builder(out_planes, out_planes, midplanes, stride),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "\n",
    "        self.conv3 = tf.keras.Sequential([\n",
    "            layers.Conv3D(\n",
    "                filters=out_planes * self.expansion,\n",
    "                kernel_size=1,\n",
    "                strides=1,\n",
    "                padding='valid',\n",
    "                use_bias=False\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "        ])\n",
    "\n",
    "        self.relu = layers.ReLU()\n",
    "\n",
    "        if downsample is None and (stride != 1 or inplanes != out_planes * self.expansion):\n",
    "            self.downsample = tf.keras.Sequential([\n",
    "                layers.Conv3D(\n",
    "                    filters=out_planes * self.expansion,\n",
    "                    kernel_size=1,\n",
    "                    strides=(stride, stride, stride),\n",
    "                    padding='valid',\n",
    "                    use_bias=False\n",
    "                ),\n",
    "                layers.BatchNormalization(),\n",
    "            ])\n",
    "        else:\n",
    "            self.downsample = downsample\n",
    "\n",
    "        self.stride = stride\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def example_conv_builder(in_planes, out_planes, midplanes, stride):\n",
    "    return layers.Conv3D(\n",
    "        filters=out_planes,\n",
    "        kernel_size=(3, 3, 3),\n",
    "        strides=(stride, stride, stride),\n",
    "        padding='same',\n",
    "        use_bias=False\n",
    "    )\n",
    "\n",
    "bottleneck_layer = Bottleneck(\n",
    "    inplanes=64,\n",
    "    out_planes=128,\n",
    "    conv_builder=example_conv_builder,\n",
    "    stride=2\n",
    ")\n",
    "\n",
    "input_tensor = tf.random.normal([1, 8, 112, 112, 64])  # Batch size , Depth , Height , Width, Channels \n",
    "output = bottleneck_layer(input_tensor)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"basic_block_1\" \"                 f\"(type BasicBlock).\n\n{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:AddV2]\n\nCall arguments received by layer \"basic_block_1\" \"                 f\"(type BasicBlock):\n  • x=tf.Tensor(shape=(1, 32, 32, 32, 64), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 65\u001b[0m\n\u001b[0;32m     57\u001b[0m block \u001b[38;5;241m=\u001b[39m BasicBlock(\n\u001b[0;32m     58\u001b[0m     inplanes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, \n\u001b[0;32m     59\u001b[0m     planes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, \n\u001b[0;32m     60\u001b[0m     conv_builder\u001b[38;5;241m=\u001b[39msimple_conv_builder\n\u001b[0;32m     61\u001b[0m )\n\u001b[0;32m     63\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n\u001b[1;32m---> 65\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[9], line 43\u001b[0m, in \u001b[0;36mBasicBlock.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[1;32m---> 43\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n\u001b[0;32m     44\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"basic_block_1\" \"                 f\"(type BasicBlock).\n\n{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:AddV2]\n\nCall arguments received by layer \"basic_block_1\" \"                 f\"(type BasicBlock):\n  • x=tf.Tensor(shape=(1, 32, 32, 32, 64), dtype=float32)"
     ]
    }
   ],
   "source": [
    "class BasicBlock(tf.keras.layers.Layer):\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        inplanes: int, \n",
    "        planes: int, \n",
    "        conv_builder, \n",
    "        stride: int = 1, \n",
    "        downsample = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Calculate midplanes similar to PyTorch implementation\n",
    "        midplanes = (inplanes * planes * 3 * 3 * 3) // (inplanes * 3 * 3 + 3 * planes)\n",
    "        \n",
    "        # Simulating Sequential with a custom conv1 method\n",
    "        self.conv1 = tf.keras.Sequential([\n",
    "            conv_builder(inplanes, planes, midplanes, stride),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU()\n",
    "        ])\n",
    "        \n",
    "        # Simulating Sequential with a custom conv2 method\n",
    "        self.conv2 = tf.keras.Sequential([\n",
    "            conv_builder(planes, planes, midplanes),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ])\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "    \n",
    "    def call(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "def simple_conv_builder(in_channels, out_channels, midplanes, stride=1):\n",
    "    return tf.keras.layers.Conv3D(\n",
    "        filters=out_channels, \n",
    "        kernel_size=3, \n",
    "        strides=stride, \n",
    "        padding='same'\n",
    "    )\n",
    "\n",
    "block = BasicBlock(\n",
    "    inplanes=64, \n",
    "    planes=128, \n",
    "    conv_builder=simple_conv_builder\n",
    ")\n",
    "\n",
    "x = tf.random.normal((1, 32, 32, 32, 64))\n",
    "\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deformable Conv Layer TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "\n",
    "class DeformableConvLayer(Conv2D):\n",
    "    \"\"\"Only support \"channel last\" data format\"\"\"\n",
    "    def __init__(self,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 strides=(1, 1),\n",
    "                 padding='same',\n",
    "                 data_format=None,\n",
    "                 dilation_rate=(1, 1),\n",
    "                 num_deformable_group=None,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"`kernel_size`, `strides` and `dilation_rate` must have the same value in both axis.\n",
    "\n",
    "        :param num_deformable_group: split output channels into groups, offset shared in each group. If\n",
    "        this parameter is None, then set  num_deformable_group=filters.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            data_format=data_format,\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            **kwargs)\n",
    "        self.kernel = None\n",
    "        self.bias = None\n",
    "        self.offset_layer_kernel = None\n",
    "        self.offset_layer_bias = None\n",
    "        if num_deformable_group is None:\n",
    "            num_deformable_group = filters\n",
    "        if filters % num_deformable_group != 0:\n",
    "            raise ValueError('\"filters\" mod \"num_deformable_group\" must be zero')\n",
    "        self.num_deformable_group = num_deformable_group\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = int(input_shape[-1])\n",
    "        # kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
    "        # we want to use depth-wise conv\n",
    "        kernel_shape = self.kernel_size + (self.filters * input_dim, 1)\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel',\n",
    "            shape=kernel_shape,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            trainable=True,\n",
    "            dtype=self.dtype)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                name='bias',\n",
    "                shape=(self.filters,),\n",
    "                initializer=self.bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "                trainable=True,\n",
    "                dtype=self.dtype)\n",
    "\n",
    "        # create offset conv layer\n",
    "        offset_num = self.kernel_size[0] * self.kernel_size[1] * self.num_deformable_group\n",
    "        self.offset_layer_kernel = self.add_weight(\n",
    "            name='offset_layer_kernel',\n",
    "            shape=self.kernel_size + (input_dim, offset_num * 2),  # 2 means x and y axis\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            trainable=True,\n",
    "            dtype=self.dtype)\n",
    "        self.offset_layer_bias = self.add_weight(\n",
    "            name='offset_layer_bias',\n",
    "            shape=(offset_num * 2,),\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            # initializer=tf.random_uniform_initializer(-5, 5),\n",
    "            regularizer=self.bias_regularizer,\n",
    "            trainable=True,\n",
    "            dtype=self.dtype)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        # get offset, shape [batch_size, out_h, out_w, filter_h, * filter_w * channel_out * 2]\n",
    "        offset = tf.nn.conv3d(inputs,\n",
    "                              filters=self.offset_layer_kernel,\n",
    "                              strides=[1, *self.strides, 1],\n",
    "                              padding=self.padding.upper(),\n",
    "                              dilations=[1, *self.dilation_rate, 1])\n",
    "        print(offset.shape, \"offset.shape\")\n",
    "        offset += self.offset_layer_bias\n",
    "\n",
    "        # add padding if needed\n",
    "        inputs = self._pad_input(inputs)\n",
    "\n",
    "        # some length\n",
    "        if inputs.get_shape()[0] is None:\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size = int(inputs.get_shape()[0])\n",
    "        channel_in = int(inputs.get_shape()[-1])\n",
    "        in_h, in_w = [int(i) for i in inputs.get_shape()[1: 3]]  # input feature map size\n",
    "        out_h, out_w = [int(i) for i in offset.get_shape()[1: 3]]  # output feature map size\n",
    "        filter_h, filter_w = self.kernel_size\n",
    "\n",
    "        # get x, y axis offset\n",
    "        offset = tf.reshape(offset, [batch_size, out_h, out_w, -1, 2])\n",
    "        y_off, x_off = offset[:, :, :, :, 0], offset[:, :, :, :, 1]\n",
    "\n",
    "        # input feature map gird coordinates\n",
    "        y, x = self._get_conv_indices([in_h, in_w])\n",
    "        y, x = [tf.expand_dims(i, axis=-1) for i in [y, x]]\n",
    "        y, x = [tf.tile(i, [batch_size, 1, 1, 1, self.num_deformable_group]) for i in [y, x]]\n",
    "        y, x = [tf.reshape(i, [*i.shape[0: 3], -1]) for i in [y, x]]\n",
    "        y = tf.cast(y, tf.float32)\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        \n",
    "        # add offset\n",
    "        y, x = y + y_off, x + x_off\n",
    "        y = tf.clip_by_value(y, 0, in_h - 1)\n",
    "        x = tf.clip_by_value(x, 0, in_w - 1)\n",
    "\n",
    "        # get four coordinates of points around (x, y)\n",
    "        y0 = tf.cast(tf.floor(y), tf.int32)\n",
    "        x0 = tf.cast(tf.floor(x), tf.int32)\n",
    "        y1, x1 = y0 + 1, x0 + 1\n",
    "        # clip\n",
    "        y0, y1 = [tf.clip_by_value(i, 0, in_h - 1) for i in [y0, y1]]\n",
    "        x0, x1 = [tf.clip_by_value(i, 0, in_w - 1) for i in [x0, x1]]\n",
    "\n",
    "        # get pixel values\n",
    "        indices = [[y0, x0], [y0, x1], [y1, x0], [y1, x1]]\n",
    "        p0, p1, p2, p3 = [DeformableConvLayer._get_pixel_values_at_point(inputs, i) for i in indices]\n",
    "\n",
    "        # cast to float\n",
    "        x0 = tf.cast(x0, tf.float32)\n",
    "        x1 = tf.cast(x1, tf.float32)\n",
    "        y0 = tf.cast(y0, tf.float32)\n",
    "        y1 = tf.cast(y1, tf.float32)\n",
    "\n",
    "\n",
    "        # weights\n",
    "        w0 = (y1 - y) * (x1 - x)\n",
    "        w1 = (y1 - y) * (x - x0)\n",
    "        w2 = (y - y0) * (x1 - x)\n",
    "        w3 = (y - y0) * (x - x0)\n",
    "        # expand dim for broadcast\n",
    "        w0, w1, w2, w3 = [tf.expand_dims(i, axis=-1) for i in [w0, w1, w2, w3]]\n",
    "        # bilinear interpolation\n",
    "        pixels = tf.add_n([w0 * p0, w1 * p1, w2 * p2, w3 * p3])\n",
    "\n",
    "        # reshape the \"big\" feature map\n",
    "        pixels = tf.reshape(pixels, [batch_size, out_h, out_w, filter_h, filter_w, self.num_deformable_group, channel_in])\n",
    "        pixels = tf.transpose(pixels, [0, 1, 3, 2, 4, 5, 6])\n",
    "        pixels = tf.reshape(pixels, [batch_size, out_h * filter_h, out_w * filter_w, self.num_deformable_group, channel_in])\n",
    "\n",
    "        # copy channels to same group\n",
    "        feat_in_group = self.filters // self.num_deformable_group\n",
    "        pixels = tf.tile(pixels, [1, 1, 1, 1, feat_in_group])\n",
    "        pixels = tf.reshape(pixels, [batch_size, out_h * filter_h, out_w * filter_w, -1])\n",
    "\n",
    "        # depth-wise conv\n",
    "        out = tf.nn.depthwise_conv2d(pixels, self.kernel, [1, filter_h, filter_w, 1], 'VALID')\n",
    "        # add the output feature maps in the same group\n",
    "        out = tf.reshape(out, [batch_size, out_h, out_w, self.filters, channel_in])\n",
    "        out = tf.reduce_sum(out, axis=-1)\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "        return self.activation(out)\n",
    "\n",
    "    def _pad_input(self, inputs):\n",
    "        \"\"\"Check if input feature map needs padding, because we don't use the standard Conv() function.\n",
    "\n",
    "        :param inputs:\n",
    "        :return: padded input feature map\n",
    "        \"\"\"\n",
    "        # When padding is 'same', we should pad the feature map.\n",
    "        # if padding == 'same', output size should be `ceil(input / stride)`\n",
    "        if self.padding == 'same':\n",
    "            in_shape = inputs.get_shape().as_list()[1: 3]\n",
    "            padding_list = []\n",
    "            for i in range(2):\n",
    "                filter_size = self.kernel_size[i]\n",
    "                dilation = self.dilation_rate[i]\n",
    "                dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
    "                same_output = (in_shape[i] + self.strides[i] - 1) // self.strides[i]\n",
    "                valid_output = (in_shape[i] - dilated_filter_size + self.strides[i]) // self.strides[i]\n",
    "                if same_output == valid_output:\n",
    "                    padding_list += [0, 0]\n",
    "                else:\n",
    "                    p = dilated_filter_size - 1\n",
    "                    p_0 = p // 2\n",
    "                    padding_list += [p_0, p - p_0]\n",
    "            if sum(padding_list) != 0:\n",
    "                padding = [[0, 0],\n",
    "                           [padding_list[0], padding_list[1]],  # top, bottom padding\n",
    "                           [padding_list[2], padding_list[3]],  # left, right padding\n",
    "                           [0, 0]]\n",
    "                inputs = tf.pad(inputs, padding)\n",
    "        return inputs\n",
    "\n",
    "    def _get_conv_indices(self, feature_map_size):\n",
    "        \"\"\"the x, y coordinates in the window when a filter sliding on the feature map\n",
    "\n",
    "        :param feature_map_size:\n",
    "        :return: y, x with shape [1, out_h, out_w, filter_h * filter_w]\n",
    "        \"\"\"\n",
    "        feat_h, feat_w = [int(i) for i in feature_map_size[0: 2]]\n",
    "\n",
    "        x, y = tf.meshgrid(tf.range(feat_w), tf.range(feat_h))\n",
    "        x, y = [tf.reshape(i, [1, *i.get_shape(), 1]) for i in [x, y]]  # shape [1, h, w, 1]\n",
    "        x, y = [tf.image.extract_patches(i,\n",
    "                                               [1, *self.kernel_size, 1],\n",
    "                                               [1, *self.strides, 1],\n",
    "                                               [1, *self.dilation_rate, 1],\n",
    "                                               'VALID')\n",
    "                for i in [x, y]]  # shape [1, out_h, out_w, filter_h * filter_w]\n",
    "        return y, x\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_pixel_values_at_point(inputs, indices):\n",
    "        \"\"\"get pixel values\n",
    "\n",
    "        :param inputs:\n",
    "        :param indices: shape [batch_size, H, W, I], I = filter_h * filter_w * channel_out\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        y, x = indices\n",
    "        batch, h, w, n = y.get_shape().as_list()[0: 4]\n",
    "\n",
    "        batch_idx = tf.reshape(tf.range(0, batch), (batch, 1, 1, 1))\n",
    "        b = tf.tile(batch_idx, (1, h, w, n))\n",
    "        pixel_idx = tf.stack([b, y, x], axis=-1)\n",
    "        return tf.gather_nd(inputs, pixel_idx)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    layer = DeformableConvLayer(32, [5, 5])\n",
    "    layer.build([16])\n",
    "    # layer.call()\n",
    "    print(layer.offset_layer_kernel)\n",
    "    # print(layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deformable Conv Layer Torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeformableConvLayer(Conv2d):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            filters,\n",
    "            kernel_size,\n",
    "            strides=(1, 1),\n",
    "            padding='same',\n",
    "            # data_format=None,\n",
    "            dilation_rate=(1,1),\n",
    "            num_deformable_group = None,\n",
    "            # activation = None,\n",
    "            use_bias = True,\n",
    "            **kwargs\n",
    "    ):\n",
    "        \"\"\"`kernel_size`, `strides` and `dilation_rate` must have the same value in both axis.\n",
    "        \n",
    "        :param num_deformable_group: split output channels into groups, offset shared in each group. If \n",
    "        this parameter is None, then  set num_deformable_group=filters.\n",
    "        \"\"\"\n",
    "        # Conv2d()\n",
    "        super().__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=strides,\n",
    "            padding=padding,\n",
    "            # data_format=data_format,\n",
    "            dilation=dilation_rate,\n",
    "            # activation=activation,\n",
    "            bias=use_bias,\n",
    "            **kwargs)\n",
    "        self.kernel = None\n",
    "        self.bias_tensor = None\n",
    "        self.offset_layer_kernel = None\n",
    "        self.offsetlayer_bias = None\n",
    "        self.use_bias = use_bias\n",
    "        if num_deformable_group is None:\n",
    "            num_deformable_group = filters\n",
    "        if filters % num_deformable_group != 0:\n",
    "            raise ValueError('\"filters\" mod \"num_deformable_group must be zero')\n",
    "        self.num_deformable_group = num_deformable_group\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        input_dim = int(input_shape[-1])\n",
    "        #kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
    "        # we want to use depth-wise conv\n",
    "        kernel_shape = self.kernel_size + (self.out_channels * input_dim, 1)\n",
    "        self.kernel = nn.Parameter(\n",
    "            torch.zeros(kernel_shape, dtype=torch.float, requires_grad=True)\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.kernel)\n",
    "        print(self.bias)\n",
    "        if self.use_bias:\n",
    "            self.bias = nn.Parameter(\n",
    "                torch.zeros(kernel_shape, dtype=torch.float),\n",
    "                requires_grad=True\n",
    "            )\n",
    "        nn.init.zeros_(self.bias)\n",
    "        \n",
    "        # create offset conv layer \n",
    "        offset_num = self.kernel_size[0] * self.kernel_size[1] * self.num_deformable_group\n",
    "        self.offset_layer_kernel = nn.Parameter(\n",
    "            torch.zeros(self.kernel_size + (input_dim, offset_num * 2), dtype=torch.float), # 2 mean x and y axis \n",
    "            requires_grad=True\n",
    "                    )\n",
    "        nn.init.zeros_(self.offset_layer_kernel)\n",
    "\n",
    "        self.offset_layer_bias = nn.Parameter(\n",
    "            torch.zeros(offset_num * 2,),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        nn.init.zeros_(self.offset_layer_bias)\n",
    "\n",
    "    def forward(self, inputs, training=None, **kwargs):\n",
    "        print(f\"inputs shape is {inputs.shape}\")\n",
    "        #get offset shape [batch_size, out_h, out_w, filter_h * filter_w * chanel_out * 2]\n",
    "        # offset = nn.Conv2d(inputs, \n",
    "        #           self.offset_layer_kernel, \n",
    "        #           bias=self.offset_layer_bias, \n",
    "        #           stride=self.strides, \n",
    "        #           padding=self.padding, \n",
    "        #           dilation=[1, self.dilation_rate, 1])\n",
    "        offset_num = self.kernel_size[0] * self.kernel_size[1] * self.num_deformable_group\n",
    "        \n",
    "        offset = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels= offset_num * 2,  # We want 2 channels for each offset (x, y)\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "            dilation=self.dilation,\n",
    "            bias=True  # We keep bias True as in TensorFlow\n",
    "        )\n",
    "\n",
    "        # Manually set the kernel weights and biases\n",
    "        offset.weight = self.offset_layer_kernel\n",
    "        offset.bias = self.offset_layer_bias\n",
    "        offset += self.offset_layer_bias\n",
    "        \n",
    "        # add padding if needed \n",
    "        inputs =  self._pad_input(inputs)\n",
    "\n",
    "        # handle batch size None\n",
    "        if input.shape[0] is None:\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size = int(inputs.shape[0])\n",
    "        channel_in = int(inputs.shape[-1])\n",
    "        in_h, in_w = [int(i) for i in input.shape[1: 3]] # input feature map size \n",
    "        out_w, out_h = [int(i) for i in offset.shape[1: 3]] # output feature map size \n",
    "        filter_h, filter_w = self.kernel_size\n",
    "\n",
    "        # get x, y axis offset \n",
    "        offset = torch.reshape(offset, [batch_size, out_h, out_w, -1, 2]) \n",
    "        y_off, x_off = offset[:, :, :, :, 0], offset[:, :, :, :, 0]\n",
    "\n",
    "        # input features \n",
    "        y, x = self._get_conv_indices([in_h, in_w])\n",
    "        y, x = [torch.tile(i, [batch_size, 1, 1, 1, self.num_deformable_group]) for i in [y, x]]\n",
    "        y, x = [torch.reshape(i, [*i.shape[0:3], -1]) for i in [y, x]]\n",
    "        y = y.to(torch.float32)\n",
    "        x = x.to(torch.float32)\n",
    "\n",
    "        # add offset\n",
    "        y, x = y + y_off, x + x_off \n",
    "        y = torch.clip(y, 0, in_h - 1)\n",
    "        x = torch.clip(x, 0, in_w - 1)\n",
    "\n",
    "        # get four coordinates of points around (x, y)\n",
    "        y0 = torch.floor(y).to(torch.int32)\n",
    "        x0 = torch.floor(x).to(torch.int32)\n",
    "        y1, x1 = y0 + 1, x0 + 1\n",
    "\n",
    "        #clip\n",
    "        y0, y1 = [torch.clip(i, 0, in_h - 1) for i in [y0, y1]]\n",
    "        x0, x1 = [torch.clip(i, 0, in_w - 1) for i in [x0, x1]]\n",
    "\n",
    "        # get pixel values \n",
    "        indices = [[y0, x0], [y0, x1], [y1, x0], [y1, x1]]\n",
    "        p0, p1, p2, p3 = [DeformableConvLayer._get_pixel_values_at_point(inputs, i) for i in indices]\n",
    "\n",
    "        # cast to float\n",
    "        x0 = x0.to(torch.float32)\n",
    "        x1 = x1.to(torch.float32)\n",
    "        y0 = y0.to(torch.float32)\n",
    "        y1 = y1.to(torch.float32) \n",
    "\n",
    "        # weights\n",
    "        w0 = (y1 - y) * (x1 - x)\n",
    "        w1 = (y1 - y) * (x - x0)\n",
    "        w2 = (y - y0) * (x1 - x)\n",
    "        w3 = (y - y0) * (x - x0)\n",
    "\n",
    "        # expand dim for broadcast\n",
    "        w0, w1, w2, w3 = [w.unsqueeze(-1) for w in [w0, w1, w2, w3]]\n",
    "\n",
    "        # bilinear interpolation\n",
    "        pixels = w0 * p0 + w1 * p1 + w2 * p2 + w3 * p3\n",
    "\n",
    "        # reshape the \"big\" feature map\n",
    "        pixels = pixels.reshape(batch_size, out_h, out_w, filter_h, filter_w, self.num_deformable_group, channel_in)\n",
    "        pixels = pixels.permute(0, 1, 3, 2, 4, 5, 6)\n",
    "        pixels = pixels.reshape(batch_size, out_h * filter_h, out_w * filter_w, self.num_deformable_group, channel_in)\n",
    "\n",
    "        # copy channels to same group\n",
    "        feat_in_group = self.filters // self.num_deformable_group\n",
    "        pixels = torch.tile(pixels, [1, 1, 1, 1, feat_in_group])\n",
    "        pixels = torch.reshape(pixels, [batch_size, out_h * filter_h, out_w * filter_w, -1])\n",
    "\n",
    "        out = torch.nn.functional.conv2d(pixels, self.kernel, stride=(filter_h, filter_w), groups=pixels.shape[3])\n",
    "\n",
    "        # add the output feature maps in the same group\n",
    "        out = torch.reshape(out, [batch_size, out_h, out_w, self.filters, channel_in])\n",
    "        out = torch.sum(out, axis=-1)\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "        return self.activation(out)\n",
    "    \n",
    "    def _pad_input(self, inputs):\n",
    "        \"\"\"Check if input feature map needs padding, because we don't use the standart Conv() function.\n",
    "\n",
    "        :param inputs:\n",
    "        :return: padded input feature map \n",
    "        \"\"\"\n",
    "\n",
    "        #When paddin is 'same', we should pad the feature map.\n",
    "        # if padding == 'same', output size should be `ceil(input / stride)`\n",
    "        if self.padding == 'same':\n",
    "            in_shape = inputs.shape[1:3]\n",
    "            padding_list = []\n",
    "            for i in range(2):\n",
    "                filter_size = self.kernel_size[i]\n",
    "                dilation = self.dilation_rate[i]\n",
    "                dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
    "                same_output = (in_shape[i] + self.strides[i] - 1) // self.strides[i]\n",
    "                valid_output = (in_shape[i] - dilated_filter_size + self.strides[i]) // self.strides[i]\n",
    "                if same_output == valid_output:\n",
    "                    padding_list += [0, 0]\n",
    "                else:\n",
    "                    p = dilated_filter_size - 1\n",
    "                    p_0 = p // 2\n",
    "                    padding_list += [p_0, p - p_0] \n",
    "    \n",
    "    def _get_conv_indices(self, feature_map_size):\n",
    "        \"\"\"the x, y coordinates in the window when a filter sliding on the feature map\n",
    "\n",
    "        :param feature_map_size:\n",
    "        :return: y, x with shape [1, out_h, out_w, filter_h * filter_w]\n",
    "        \"\"\"\n",
    "        feat_h, feat_w = [int(i) for i in feature_map_size[0: 2]]\n",
    "\n",
    "        x, y = torch.meshgrid(torch.range(feat_w),  torch.range(feat_h))\n",
    "        x, y = [torch.reshape(i, [1, *i.shape, 1]) for i in [x, y]]\n",
    "        x, y = [nn.functional.unfold(i, \n",
    "                                     kernel_size=[1, *self.kernel_size, 1],\n",
    "                                     stride=[1, *self.strides, 1],\n",
    "                                     dilation=[1, *self.dilation_rate, 1],\n",
    "                                     padding=0) \n",
    "                                     for i in [x, y]]\n",
    "        print(f\"Shapes after unfold. x.shape {x.shape} y.shape {y.shape}\")\n",
    "        return y, x\n",
    "    \n",
    "    def _get_pixel_values_at_point(inputs, indices):\n",
    "        \"\"\"get pixel values\n",
    "\n",
    "        :param inputs:\n",
    "        :param indices: shape [batch_size, H, W, I], I = filter_h * filter_w * chanel_out\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        y, x = indices \n",
    "        batch, h, w, n = y.shape[0: 4] \n",
    "\n",
    "        batch_idx = torch.reshape(torch.range(0, batch), (batch, 1, 1, 1))\n",
    "        b = torch.tile(batch_idx, (1, h, w, n))\n",
    "        print(f\"b.shape is {b.shape}\")\n",
    "        pixel_idx = torch.stack([b, y, x], dim=-1)\n",
    "        print(f\"pixel index {pixel_idx}\")\n",
    "        return  torch.index_select(inputs, pixel_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randint(0, 3, (112, 112, 3))\n",
    "Conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.0258,  0.0370, -0.1138, -0.0252, -0.0704,  0.1035,  0.0233,  0.1130,\n",
      "         0.0458, -0.0329, -0.0951, -0.0125, -0.0268, -0.0197, -0.0618, -0.0790,\n",
      "         0.0018, -0.0489, -0.0547,  0.0347, -0.0726, -0.1150,  0.0094, -0.0300,\n",
      "         0.0680,  0.0602,  0.0736,  0.0996,  0.0058, -0.0634,  0.0613,  0.0226],\n",
      "       requires_grad=True)\n",
      "inputs shape is torch.Size([112, 112, 3])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Conv2d' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m layer \u001b[38;5;241m=\u001b[39m DeformableConvLayer(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m32\u001b[39m, [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m])\n\u001b[0;32m      2\u001b[0m layer\u001b[38;5;241m.\u001b[39mbuild([\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(layer\u001b[38;5;241m.\u001b[39moffset_layer_kernel)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(layer)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[108], line 100\u001b[0m, in \u001b[0;36mDeformableConvLayer.forward\u001b[1;34m(self, inputs, training, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m offset\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset_layer_kernel\n\u001b[0;32m     99\u001b[0m offset\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset_layer_bias\n\u001b[1;32m--> 100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moffset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset.shape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m offset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset_layer_bias\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# add padding if needed \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Conv2d' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "layer = DeformableConvLayer(3, 32, [5, 5])\n",
    "layer.build([20, 10, 3])\n",
    "layer(input_tensor)\n",
    "print(layer.offset_layer_kernel)\n",
    "# print(layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
