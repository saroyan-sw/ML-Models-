{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Conv2d\n",
    "from torch import Tensor\n",
    "from typing import Any, Callable, List, Optional, Sequence, Tuple, Type, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2Plus1D(nn.Sequential):\n",
    "    def __init__(self, in_planes: int, out_planes: int, midplanes: int, stride: int = 1, padding: int = 1) -> None:\n",
    "        super().__init__(\n",
    "            nn.Conv3d(\n",
    "                in_planes,\n",
    "                midplanes,\n",
    "                kernel_size=(1, 3, 3),\n",
    "                stride=(1, stride, stride),\n",
    "                padding=(0, padding, padding),\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm3d(midplanes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(\n",
    "                midplanes, out_planes, kernel_size=(3, 1, 1), stride=(stride, 1, 1), padding=(padding, 0, 0), bias=False\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_downsample_stride(stride: int) -> Tuple[int, int, int]:\n",
    "        return stride, stride, stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2Plus1dStem(nn.Sequential):\n",
    "    \"\"\"R(2+1)D stem is different than the default one as it uses separated 3D convolution\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\n",
    "            nn.Conv3d(3, 45, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False),\n",
    "            nn.BatchNorm3d(45),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(45, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        out_planes: int,\n",
    "        conv_builder: Callable[..., nn.Module],\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        midplanes = (inplanes * out_planes * 3 * 3 * 3) // (inplanes * 3 * 3 + 3 * out_planes)\n",
    "\n",
    "        # 1x1x1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(inplanes, out_planes, kernel_size=1, bias=False), \n",
    "            nn.BatchNorm3d(out_planes), \n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Second kernel\n",
    "        self.conv2 = nn.Sequential(\n",
    "            conv_builder(out_planes, out_planes, midplanes, stride), \n",
    "            nn.BatchNorm3d(out_planes), \n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # 1x1x1\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv3d(out_planes, out_planes * self.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm3d(out_planes * self.expansion),\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        conv_builder: Callable[..., nn.Module],\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "    ) -> None:\n",
    "        midplanes = (inplanes * planes * 3 * 3 * 3) // (inplanes * 3 * 3 + 3 * planes)\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            conv_builder(inplanes, planes, midplanes, stride), \n",
    "            nn.BatchNorm3d(planes), \n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(conv_builder(planes, planes, midplanes), nn.BatchNorm3d(planes))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoResNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            block: Type[Union[BasicBlock, Bottleneck]],\n",
    "            conv_makers: Sequence[Type[Union[Conv2Plus1D]]],\n",
    "            nums_of_layers: List[int], \n",
    "            stem: Callable[..., nn.Module],\n",
    "            num_classes: int = 2,\n",
    "            zero_init_residual: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.inplanes = 64\n",
    "\n",
    "        self.stem = stem()\n",
    "\n",
    "        self.layer1 = self._make_layer(block, conv_makers[0], 64, nums_of_layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, conv_makers[1], 128, nums_of_layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, conv_makers[2], 256, nums_of_layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, conv_makers[3], 512, nums_of_layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        #init weights \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "        \n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.stem(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        #flatten layer to fc\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _make_layer(\n",
    "            self,\n",
    "            block: Type[Union[BasicBlock, Bottleneck]],\n",
    "            conv_builder: Type[Union[Conv2Plus1D]],\n",
    "            out_planes: int,\n",
    "            num_of_blocks: int,\n",
    "            stride: int = 1,\n",
    "            ) -> nn.Sequential:\n",
    "        downsample = None\n",
    "\n",
    "        if stride != 1 or self.inplanes != out_planes *  block.expansion:\n",
    "            ds_stride = conv_builder.get_downsample_stride(stride)\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv3d(self.inplanes, out_planes*block.expansion, kernel_size=1, stride=ds_stride, bias=False),\n",
    "                nn.BatchNorm3d(out_planes * block.expansion)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, out_planes, conv_builder, stride, downsample))\n",
    "\n",
    "        self.inplanes = out_planes * block.expansion\n",
    "        for i in range(1, num_of_blocks):\n",
    "            layers.append(block(self.inplanes, out_planes, conv_builder))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video_resnet(\n",
    "    block: Type[Union[BasicBlock, Bottleneck]],\n",
    "    conv_makers: Sequence[Type[Union[Conv2Plus1D]]],\n",
    "    layers: List[int],\n",
    "    stem: Callable[..., nn.Module],\n",
    ") -> VideoResNet:\n",
    "\n",
    "    model = VideoResNet(block, conv_makers, layers, stem)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoResNet(\n",
       "  (stem): R2Plus1dStem(\n",
       "    (0): Conv3d(3, 45, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False)\n",
       "    (1): BatchNorm3d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv3d(45, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 230, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(230, 128, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(128, 230, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(230, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(128, 288, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(288, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(128, 288, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(288, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(128, 460, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(460, 256, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(256, 460, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(460, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(256, 576, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(576, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(256, 576, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(576, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(256, 921, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(921, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(921, 512, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(512, 921, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(921, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(921, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(512, 1152, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(1152, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(512, 1152, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(1152, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_video_resnet(\n",
    "        BasicBlock,\n",
    "        [Conv2Plus1D] * 4,\n",
    "        [2, 2, 2, 2],\n",
    "        R2Plus1dStem,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deformable Conv Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (95916157.py, line 67)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 67\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.bias = self.\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class DeformableConvLayer(Conv2d):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            filters,\n",
    "            kernel_size,\n",
    "            strides=(1, 1),\n",
    "            padding='same',\n",
    "            data_format=None,\n",
    "            dilation_rate=(1,1),\n",
    "            num_deformable_group = None,\n",
    "            activation = None,\n",
    "            use_bias = None,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            bias_initializer='zeros',\n",
    "            kernel_regularizer=None,\n",
    "            bias_regularizer=None,\n",
    "            activity_regularizer=None,\n",
    "            kernel_constraint=None,\n",
    "            bias_constraint=None,\n",
    "            **kwargs\n",
    "    ):\n",
    "        \"\"\"`kernel_size`, `strides` and `dilation_rate` must have the same value in both axis.\n",
    "        \n",
    "        :param num_deformable_group: split output channels into groups, offset shared in each group. If \n",
    "        this parameter is None, then  set num_deformable_group=filters.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            data_format=data_format,\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            **kwargs)\n",
    "        self.kernel = None\n",
    "        self.bias = None\n",
    "        self.offset_layer_kernel = None\n",
    "        self.offsetlayer_bias = None\n",
    "        if num_deformable_group is None:\n",
    "            num_deformable_group = filters\n",
    "        if filters % num_deformable_group != 0:\n",
    "            raise ValueError('\"filters\" mod \"num_deformable_group must be zero')\n",
    "        self.num_deformable_group = num_deformable_group\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        input_dim = int(input_shape[-1])\n",
    "        #kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
    "        # we want to use depth-wise conv\n",
    "        kernel_shape = self.kernel_size + (self.filters * input_dim, 1)\n",
    "        self.kernel = nn.Parameter(\n",
    "            torch.zeros(kernel_shape, dtype=torch.float, requires_grad=True)\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.kernel)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = nn.Parameter(\n",
    "                torch.zeros(kernel_shape, dtype=torch.float),\n",
    "                requires_grad=True\n",
    "            )\n",
    "        nn.init.zeros_(self.bias)\n",
    "        \n",
    "        # create offset conv layer \n",
    "        offset_num = self.kernel_size[0] * self.kernel_size[1] * self.num_deformable_group\n",
    "        self.offset_layer_kernel = nn.Parameter(\n",
    "            torch.zeros(self.kernel_size + (input_dim, offset_num * 2), dtype=torch.float), # 2 mean x and y axis \n",
    "            requires_grad=True\n",
    "                    )\n",
    "        nn.init.zeros_(self.offset_layer_kernel)\n",
    "\n",
    "        self.offset_layer_bias = nn.Parameter(\n",
    "            torch.zeros(offset_num * 2,),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        nn.init.zeros_(self.offset_layer_bias)\n",
    "\n",
    "    def forward(self, inputs, training=None, **kwargs):\n",
    "        #get offset shape [batch_size, out_h, out_w, filter_h * filter_w * chanel_out * 2]\n",
    "        offset = nn.Conv2d(inputs, \n",
    "                  self.offset_layer_kernel, \n",
    "                  bias=self.offset_layer_bias, \n",
    "                  stride=self.strides, \n",
    "                  padding=self.padding, \n",
    "                  dilation=[1, self.dilation_rate, 1])\n",
    "        print(offset.shape, \"offset.shape\")\n",
    "        offset += self.offset_layer_bias\n",
    "        ... #Continue \n",
    "\n",
    "    \n",
    "    def _pad_input(self, inputs):\n",
    "        \"\"\"Check if input feature map needs padding, because we don't use the standart Conv() function.\n",
    "\n",
    "        :param inputs:\n",
    "        :return: padded input feature map \n",
    "        \"\"\"\n",
    "\n",
    "        #When paddin is 'same', we should pad the feature map.\n",
    "        # if padding == 'same', output size should be `ceil(input / stride)`\n",
    "        if self.padding == 'same':\n",
    "            in_shape = inputs.shape[1:3]\n",
    "            padding_list = []\n",
    "            for i in range(2):\n",
    "                filter_size = self.kernel_size[i]\n",
    "                dilation = self.dilation_rate[i]\n",
    "                dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
    "                same_output = (in_shape[i] + self.strides[i] - 1) // self.strides[i]\n",
    "                valid_output = (in_shape[i] - dilated_filter_size + self.strides[i]) // self.strides[i]\n",
    "                if same_output == valid_output:\n",
    "                    padding_list += [0, 0]\n",
    "                else:\n",
    "                    p = dilated_filter_size - 1\n",
    "                    p_0 = p // 2\n",
    "                    padding_list += [p_0, p - p_0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv2d()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
